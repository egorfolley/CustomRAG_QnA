{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5443e62b",
   "metadata": {},
   "source": [
    "# Playground for custom RAG system\n",
    "____\n",
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d621b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df6c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\"MISTRAL_API_KEY not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0d8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into overlapping chunks (simple word-based chunking)\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4535755",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"data/test/Custom_RAG_Test_Document.pdf\"\n",
    "if not pdf_file.endswith('.pdf'):\n",
    "    raise ValueError(\"File must be a PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b89f6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom RAG Test Document\n",
      "1. Company Overview\n",
      "Acme AI is a fictional enterprise software company specializing in AI agent deployment platforms\n",
      "for mid-sized businesses. The company was founded in 2021 and is headquartered in New York.\n",
      "2. Financial Summary (2025)\n",
      "Revenue: $18 million. Gross Margin: 72%. Net Income: -$2.4 million. Customer Growth Rate: 38%\n",
      "year-over-year.\n",
      "3. Competitive Landscape\n",
      "Primary competitors include StackAI, Credal, and Glean. Acme AI differentiates itself through rapid\n",
      "deployment cycles and strong enterprise security compliance.\n",
      "4. Macro Environment Context\n",
      "In 2025, tightening credit conditions and elevated interest rates impacted enterprise software\n",
      "spending. However, AI infrastructure investments continued due to productivity gains and\n",
      "automation trends.\n",
      "5. Risk Factors\n",
      "Key risks include increased competition, dependency on venture funding, and potential slowdown in\n",
      "SaaS budgets if macroeconomic conditions deteriorate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8556e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 chunks\n"
     ]
    }
   ],
   "source": [
    "chunk_texts = chunk_text(text)\n",
    "print(f\"Created {len(chunk_texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478e407",
   "metadata": {},
   "source": [
    "### Creating embeddings (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1f14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from app.config import MISTRAL_API_KEY, CHUNK_SIZE, CHUNK_OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb65b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralClient(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "# Global storage (in-memory)\n",
    "chunks = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfecbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunks(text_chunks):\n",
    "    \"\"\"Get embeddings from Mistral API\"\"\"\n",
    "    response = client.embeddings(\n",
    "            model=\"mistral-embed\",\n",
    "            input=text_chunks\n",
    "        )\n",
    "    data = response.data\n",
    "\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "# Testing full ingetion pipeline\n",
    "def ingest_pdf(pdf_file):\n",
    "    \"\"\"Main ingestion pipeline\"\"\"\n",
    "        \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    print(f\"Extracted text length: {len(text)} characters\")\n",
    "    \n",
    "    # Chunk text\n",
    "    new_text_chunks = chunk_text(text)\n",
    "    print(f\"Created {len(new_text_chunks)} chunks\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    new_embeddings = embed_chunks(new_text_chunks)\n",
    "    print(f\"Generated embeddings for {len(new_embeddings)} chunks\")\n",
    "    \n",
    "    # Store\n",
    "    chunks.extend(new_text_chunks)\n",
    "    embeddings.extend(new_embeddings)\n",
    "    \n",
    "    return len(new_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e37d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 1 chunks\n"
     ]
    }
   ],
   "source": [
    "embed_chunks = embed_chunks(chunk_texts)\n",
    "print(f\"Generated embeddings for {len(embed_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e2c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text length: 958 characters\n",
      "Created 1 chunks\n",
      "Generated embeddings for 1 chunks\n",
      "Ingested 1 chunks from PDF\n"
     ]
    }
   ],
   "source": [
    "num_chunks = ingest_pdf(pdf_file)\n",
    "print(f\"Ingested {num_chunks} chunks from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e52c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
